<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 8)|!(IE)]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
   <title>Kathlén Kohn</title>
   <meta name="description" content="">
   <meta name="author" content="">

   <!-- Mobile Specific Metas
   ================================================== -->
   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

   <!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/default.css">
   <link rel="stylesheet" href="css/layout.css">
   <link rel="stylesheet" href="css/media-queries.css">
   <link rel="stylesheet" href="css/magnific-popup.css">

   <!-- Script
   ================================================== -->
   <script src="js/modernizr.js"></script>

   <!-- Favicons
	================================================== -->
   <link rel="shortcut icon" href="infinity.png">

</head>

<body>

   <!-- Header
   ================================================== -->
   <section id="education">

      <nav id="nav-wrap">

         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">Show navigation</a>
         <a class="mobile-btn" href="#" title="Hide navigation">Hide navigation</a>

         <ul id="nav" class="nav">
            <li class="current"><a href="index.html">Home</a></li>
            <li class="current"><a href="index.html"></a></li>
         </ul> <!-- end #nav -->

      </nav> <!-- end #nav-wrap -->

      <div class="row education">

         <div class="nine columns main-col">

            <div class="row item">

               <div class="twelve columns">

                  <h1 class='h1-header'>2 postdoctoral positions in algebraic geometry applied to machine learning & computer vision</h1>
		  <p class="info">KTH Royal Institute of Technology, Stockholm</p>
                  <p class="info">Application deadline <span>&bull;</span> <em class="date">TBA</em></p>
		  <p class="info">How to apply<span>&bull;</span> <em class="date">TBA
			  <!--<a class='link-in-text' href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:371546">go here</a> and click on <a class='link-in-text' href="https://kth.varbi.com/en/what:login/jobID:371546/type:job/apply:1/">Login and apply</a> at very bottom of the page-->
		  </em></p>
		  <p class="info">Contact <span>&bull;</span> <em class="date"> 
			  <a class='link-in-text' href="https://kathlenkohn.github.io/">Kathlén Kohn</a> kathlen@kth.se </em></p>
                  <p class='classic-paragraph'>
			  The research group on Applied Algebraic Geometry in Data Science and AI led by Kathlén Kohn at KTH Stockholm has two open postdoctoral positions:
			  <ol>
 				<li><b>Postdoc Fellow:</b> employment for 2 years <br> Requirement: a doctoral degree or an equivalent foreign degree, obtained within the last three years prior to the application deadline (with exceptions for special reasons, e.g., sick or parental leave) </li>
				<li><b>Researcher:</b> employment for 1 year </li>
			</ol>  
                  </p>


                  <h2 class='h2-header'>The theme of the possible research projects is <span
                        class='inner-header'>Convergence in
                        Learning</span>.</h2>


                  <p class='classic-paragraph'>
                     Many machine learning models rely on optimization procedures that fit
                     various parameters to a
                     given
                     training dataset.
                     While the convergence behavior of classical methods (such as shrinkage models, decision trees, and
                     support vector machines) is relatively well understood, less is known for deeper models, such as
                     neural networks.
                     A similar problem occurs for models based on low-rank matrix factorization.
                     Both methods involve non-convex optimization and convergence to global minima is not guaranteed.
                     Nonetheless, neural networks and low-rank matrix factorization achieve impressive results in a wide
                     variety of tasks.
                     Understanding the convergence behavior of these and characterizing the resulting models is
                     therefore
                     of great importance.
                     Recent work has started illuminating these questions for specific classes of neural networks and
                     matrix factorization methods.
                     Other work has focused on characterizing the properties of the networks once converged.
                     The successful candidate will work on extending these results by combining ideas from algebraic
                     geometry and statistical signal processing, while anchoring the theoretical analysis in concrete
                     problems in computer vision and cryogenic electron microscopy (cryo-EM).
                     One of the following projects, each illuminating different aspects of the above problem, possibly
                     in
                     combination with one or two side projects, will form the core research subject of a doctoral
                     thesis.
                  </p>

                  <h3 class='h3-header'>Scattering transforms </h3>
                  <img class='scalogram' src='images/speech_scalograms.png' alt='scalogram for 2 speech signals'></img>
                  <p class='classic-paragraph'>
                     These transforms were introduced as convolutional networks with fixed weights that guarantee
                     certain
                     invariance and stability properties with respect to translation and deformation.
                     By explicitly encoding these symmetries, networks consisting of scattering transforms followed by a
                     linear layer have achieved significant success in various classification and regression tasks.
                     Since only a single linear layer is optimized, the problem becomes convex and a global optimum is
                     easily found.
                     In addition, the fixed structure of the scattering transform and the relative simplicity of the
                     linear
                     layer simplifies analysis and increases interpretability of the model.
                     Understanding these networks therefore amounts to characterizing the space of linear combinations
                     of
                     scattering transform coefficients.
                     Determining the properties of this space would specify the limitations of these networks and
                     suggest
                     extensions that would increase their expressivity.
                  </p>

                  <h3 class='h3-header'>Attractors of autoencoders </h3>
                  <img class='attractors' src='images/iterations_attractors.png' alt='attractors'></img>
                  <p class='classic-paragraph'>
                     An important aspect of autoencoders is their ability to memorize the training data, which has been
                     recently explored from a dynamical systems perspective.
                     Empirical results suggest that training examples form attractors of these autoencoders, but the
                     theoretical reasons behind that mechanism are still not clear.
                     Algebraic techniques can be applied in the setting of ReLU autoencoders with Euclidean loss, as the
                     underlying geometric problem is to find a closest point on a semi-algebraic set.
                     An open conjecture is that all training examples are attractors in a (global) minimum of the
                     Euclidean
                     loss of a sufficiently deep ReLU autoencoder.
                     This project would investigate that conjecture as well as further conditions under which attractors
                     are formed.
                  </p>
                  <h3 class='h3-header'>Convergence of linear networks </h3>
                  <img class='ellipse' src='images/ellipse_caustic.svg' alt='caustic of an ellipse'></img>
                  <p class='classic-paragraph'>
                     As linear networks are the easiest type of neural networks, many of their properties are well
                     understood, including the structure of their critical points when using the Euclidean loss
                     function.
                     However, it remains an open problem to show that generic initializations of the network converge
                     under
                     gradient flow to a global minimum.
                     Another project is therefore to investigate this conjecture and possibly expand it to other types
                     of
                     networks or loss functions.
                  </p>
                  <h3 class='h3-header'>Convergence in low-rank matrix factorization models </h3>
                  <p class='classic-paragraph'>
                     <img class='ribosome' src='images/rib80s_var.png' alt="variablity map for an 80's ribosome"></img>
                     A similar behavior is observed in low-rank matrix models.
                     Recent work has shown that global convergence is possible under certain settings of linear matrix
                     measurement using Gaussian matrices.
                     However, this convergence behavior is observed for much wider classes of measurement operators.
                     In particular, replacing the Gaussian sensing matrices with certain integral operators representing
                     tomographic projection, global convergence is observed in a wide range of configurations.
                     This particular setup has applications to the heterogeneity problem in cryo-EM, where a low-rank
                     factorization model can be used to characterize the structural variability of three-dimensional
                     density maps representing the imaged molecule.
                     This project would investigate this behavior and extend previous convergence results to wider
                     settings
                     of measurement operators.
                  </p>





               </div>

            </div> <!-- item end -->

         </div> <!-- main-col end -->

      </div> <!-- End -->

   </section> <!-- Header End -->



   <!-- footer
   ================================================== -->
   <footer>

      <div class="row">

         <div class="twelve columns">

            <ul class="copyright">
               <li>Design by <a title="Styleshout" href="http://www.styleshout.com/">Styleshout</a></li>
            </ul>

         </div>


      </div>

   </footer> <!-- Footer End-->

   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>

   <script src="js/jquery.flexslider.js"></script>
   <script src="js/waypoints.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/magnific-popup.js"></script>
   <script src="js/init.js"></script>

</body>

</html>


<!-- 
 -->
